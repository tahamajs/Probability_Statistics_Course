{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477fe80afd086490",
   "metadata": {},
   "source": [
    "# Book Categorization Using Bayesian Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "![Python Logo](Screenshot%202023-10-22%20at%208.30.57%20PM.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82ed786f6bb5c72",
   "metadata": {},
   "source": [
    "#### Mohammad Taha Majlesi 810101504\n",
    "## probebility and statics project 1\n",
    "\n",
    "baysian classifier for categorizing books\n",
    "for running this project it takes over 10 minutes to run all of cells\n",
    "but i write main results in cells and you can run them and see results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955243b0d9888021",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Readind Training Date from CSV\n",
    "first we need to read data from csv train dataset and show it for better understanding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e484350a3465ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Train_data_frame = pd.read_csv('books_train.csv')\n",
    "Train_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44779d75356636",
   "metadata": {},
   "source": [
    "\n",
    "***we make new dataframe that name is category_data_frame***\n",
    "we need to make new dataframe that has 3 columns\n",
    "1. categories\n",
    "2. data\n",
    "3. BOW\n",
    "and then we need to put all of data in one row for each category\n",
    "\n",
    "\n",
    "that data frame name is category_data_frame\n",
    "we add all of description and title of each category in one row\n",
    "by that we have 6 rows \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98ecc32410ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_data_frame = pd.DataFrame(columns=['categories','data','BOW'])\n",
    "\n",
    "category_data_frame['categories'] = Train_data_frame['categories'].unique()\n",
    "\n",
    "category_data_frame['data'] = ' '.join(Train_data_frame['description'])\n",
    "for i in category_data_frame['categories']:\n",
    "    category_data_frame['data'][category_data_frame['categories'] == i] = ' '.join(Train_data_frame['description'][Train_data_frame['categories'] == i] + ' ' + Train_data_frame['title'][Train_data_frame['categories'] == i])\n",
    "\n",
    "\n",
    "category_data_frame\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177020c27bdc3c0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## phase 1\n",
    "# Preprocessing Data\n",
    "\n",
    "we need function for removing numbers and special characters and then we need to tokenize data\n",
    "we use HAZM library for that and regex library for removing special characters and numbers\n",
    "and then we need to tokenize that data\n",
    "\n",
    "for step words we use hazm library step words and then we add more step words to that list\n",
    "and additional step words are in json file we add that to list and then we use that list for removing step words from data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a1a1110a22e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_step_words = pd.read_json('step_words.json')\n",
    "print(len(json_step_words) )\n",
    "more_step_words = list(json_step_words['stopWords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6b9ea7ea35dd81",
   "metadata": {},
   "source": [
    "\n",
    "__we have tow function for preprocessing data one with step words and one without step words for comparing results__\n",
    "useing data = re.sub(r'[^\\w\\s]','',data) is for removing special characters and useing data = re.sub(r'\\d+','',data) is for removing numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276e36b432d5ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "import re\n",
    "\n",
    "def preprocess_with_step_word(data):\n",
    "    step_words = stopwords_list()\n",
    "    step_words += more_step_words\n",
    "\n",
    "    normalizer = Normalizer()\n",
    "    # normalizing data\n",
    "    data = normalizer.normalize(data)\n",
    "\n",
    "    # removing special characters\n",
    "    data = re.sub(r'[^\\w\\s]','',data)\n",
    "    \n",
    "    # numbers and /n removing\n",
    "    data = re.sub(r'\\d+','',data)\n",
    "    data = re.sub(r'\\n+',' ',data)\n",
    "    data = word_tokenize(data)\n",
    "    data = [item for item in data if item not in step_words]\n",
    "    # data = [Stemmer().stem(item) for item in data]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2ece23fe9febe4",
   "metadata": {},
   "source": [
    "now we preprocess data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2c9581c2e684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_data_frame['data'] = category_data_frame['data'].apply(lambda x: preprocess_with_step_word(x))\n",
    "' '.join(category_data_frame['data'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445fbfb77d71071",
   "metadata": {},
   "source": [
    "## show data and plot it after normalizing and removing special characters and numbers and removing step words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b82fbb86d64a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(category_data_frame['categories'],[len(category_data_frame['data'][i]) for i in range(len(category_data_frame['data']))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa09398997bb723",
   "metadata": {},
   "source": [
    "### show data numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d85d4a36d636c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(category_data_frame['data'])):\n",
    "    print(category_data_frame['categories'][i],len(category_data_frame['data'][i]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e994577b4390923",
   "metadata": {},
   "source": [
    "## MAKE BAG OF WORDS\n",
    "we need to make bag of words for each category and assign each word to a number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20255f819f896336",
   "metadata": {},
   "source": [
    "\n",
    "### Making Unique Words of all of words in trained data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd82572b9f496d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe for unique words\n",
    "unique_words = pd.DataFrame(columns=['words'])\n",
    "all_train_data = ( ' '.join([i for i in Train_data_frame['categories']]))\n",
    "all_train_data = all_train_data + ' '.join([i for i in Train_data_frame['title']])\n",
    "all_train_data = preprocess_with_step_word(all_train_data)\n",
    "all_train_data = list(set(all_train_data))\n",
    "unique_words['words'] = all_train_data\n",
    "len(unique_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeecf301df6d009",
   "metadata": {},
   "source": [
    "__this show that we have around 4000 unique words in all of data set__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d671d944db879fa",
   "metadata": {},
   "source": [
    "## Make Bag of Words with unique words\n",
    "for all of categories we need to calculate how many times each word is in each category\n",
    "در اینجا ما یک ارایه از اعداد که ظرفیت ۵۰۰۰۰ تایی دارند را به قسمت BOW در دیتا فریم اضافه میکنیم \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc46c422a0c4241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(category_data_frame['categories'])):\n",
    "    BOW = [0] * len(unique_words)\n",
    "\n",
    "\n",
    "    for index, word in enumerate(unique_words['words']):\n",
    "        if word in category_data_frame['data'][i]:\n",
    "            count = category_data_frame['data'][i].count(word)\n",
    "            BOW[index] += count\n",
    "    category_data_frame['BOW'][i] = BOW\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563701a18fc19292",
   "metadata": {},
   "source": [
    "## make plot of words in each category for 1 - 1000\n",
    "\n",
    "صرفا برای نشان دادن تجمع کلمات در هر کدام از دسته ها "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f0d89b858779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(unique_words['words'][:1000],category_data_frame['BOW'][0][:1000])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(unique_words['words'][:1000],category_data_frame['BOW'][1][:1000])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(unique_words['words'][:1000],category_data_frame['BOW'][2][:1000])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(unique_words['words'][:1000],category_data_frame['BOW'][3][:1000])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(unique_words['words'][:1000],category_data_frame['BOW'][4][:1000])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(unique_words['words'][:1000],category_data_frame['BOW'][5][:1000])\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e769c0775b1f4d43",
   "metadata": {},
   "source": [
    "## Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df566a8f0ef069a",
   "metadata": {},
   "source": [
    "## Making Prediction Function\n",
    "we need to make prediction function for each word in each category\n",
    "we use Bayes rule for that\n",
    "P(C|X) = P(X|C) * P(C) / P(X)\n",
    "P(C|X) = probability of category C when we have X\n",
    "P(X|C) = probability of X when we have category C\n",
    "P(C) = probability of category C\n",
    "P(X) = probability of X\n",
    "we need to calculate P(X|C) for each word in each category\n",
    "and then we need to calculate P(C) for each category\n",
    "but we know that p(C) is same 1/6 for each category\n",
    "so we dont need to calculate that\n",
    "and because of that we want to compare P(X|C) for each category and then we choose category that has higher probability we dont need to calculate P(X) because it is same for each category\n",
    "and just we need to calculate P(X|C) for each category\n",
    "and then we need to multiply all of P(xi|C) for each word in each category\n",
    "because we have more than one word in each example and probebility of each word is independent from other words\n",
    "so we need to multiply all of P(xi|C) for each word in each category\n",
    "for calculate P(X|C) we need to calculate P(xi|C) for each word in each category\n",
    "P(X|C) = P(x1|C) * P(x2|C) * P(x3|C) * ... * P(xn|C)\n",
    "and then we use log for that because we have very small numbers and we have underflow\n",
    "so we use log for that\n",
    "log(P(X|C)) = log(P(x1|C)) + log(P(x2|C)) + log(P(x3|C)) + ... + log(P(xn|C))\n",
    "\n",
    "\n",
    "if we do not have a word in a category we have 0 for that word in that category and it is not correct\n",
    "so we need to use additive smoothing for that\n",
    "we add 1 to each word in each category\n",
    "so we have 1 for each word in each category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f31f5bbe644ca4",
   "metadata": {},
   "source": [
    "## making prediction function for one word in one category \n",
    "\n",
    "\n",
    "٢‐ فرض کنید در شرایطی متن توضیحات یک کتاب طولانی باشد در این صورت با ضرب شدن احتمالهای هر کلمه چه\n",
    "اتفاقی می افتد؟ پیشنهاد شما برای رفع این مشکل چیست؟\n",
    "\n",
    "همان گونه که گفته شد از لوگاریتم استفاده میکنیم یعنی لوگاریتم ها را باید با هم جمع کنیم \n",
    "در تابع پایین خروجی لوگاریتم است \n",
    "\n",
    "\n",
    "در این قسمت ما تابعی برای احتمال وجود هر کلمه داریم \n",
    "چون که احتمال وجود هر کلمه از تقسیم کردن مقدار موجود در بگ او ورد بر تعداد کل کلمات است پس اگر به طور عادی بخواهیم برای هر کدارم از کلمات این مقدارهارا در هم ضرب کنیم عددی بسیار کوچک داریم .\n",
    "بنابر این از تابع لوگاریتم استفاده میکنیم .\n",
    "یعنی باید برای تک تک کلمات لوگاریتم های ان هارا با هم جمع کنیم .\n",
    "\n",
    "\n",
    "دو تا تابع برای پیشبینی داریم یکی با اسم additive smoothing که اگر کلمه ای در دسته ای وجود نداشته باشد به جای صفر یک میگذارد و یک تابع دیگر بدون اسم که اگر کلمه ای در دسته ای وجود نداشته باشد صفر را بر میگرداند .\n",
    "\n",
    "\n",
    "if we have a word that is not in unique words we have 0 for that word in each category and we dont have any prediction for that word in each category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68780aa7cc6dafb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb7f300b56738b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "def predict_without_additive_smooth(word,category):\n",
    "    if word not in list(unique_words['words']):\n",
    "        return 0\n",
    "    index = unique_words[unique_words['words'] == word].index[0]\n",
    "    if category_data_frame['BOW'][category][index] == 0:\n",
    "        return 0\n",
    "    return log(category_data_frame['BOW'][category][index] / sum(category_data_frame['BOW'][category]))\n",
    "\n",
    "\n",
    "def predict_with_additive_smooth(word,category):\n",
    "    if word not in list(unique_words['words']):\n",
    "        return 0\n",
    "    index = unique_words[unique_words['words'] == word].index[0]\n",
    "    if category_data_frame['BOW'][category][index] == 0:\n",
    "        return log(1/(sum(category_data_frame['BOW'][category])+1))\n",
    "    return log(category_data_frame['BOW'][category][index]/sum(category_data_frame['BOW'][category]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a341c7502f87056",
   "metadata": {},
   "source": [
    "## test prediction function\n",
    "this be negative prediction JUST simple test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b654f3908e2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_with_additive_smooth('کتاب',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55b87767d599274",
   "metadata": {},
   "source": [
    "for predicting one sentence we need to sum all of words in that sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5083c3d717e5455",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = \"\"\"«قلعه‌ی حیوانات» جورج اورول، درباره گروهی از حیوانات اهلی است که در اقدامی آرمان‌گرایانه و انقلابی، صاحب مزرعه (آقای جونز) را از مزرعه‌اش فراری می‌دهند تا خودشان اداره مزرعه را به‌دست گرفته و «برابری» و «رفاه» را در جامعه خود برقرار سازند. رهبری این جنبش را گروهی از خوک‌ها به‌دست دارند ولی پس از مدتی این گروه جدید نیز به رهبری خوکی به نام ناپلئون، به بهره‌کشی از حیوانات مزرعه می‌پردازند و هرگونه مخالفتی را سرکوب می‌کنند.\n",
    "اورول به عنوان یک سوسیال- دموکرات در جریان جنگ داخلی اسپانیا با سیاست‌های حکومت سوسیالیستی شوروی آشنا شده‌بود و از پاکسازی‌های خشونت‌آمیز دوران استالین خشمگین بود. او با نگارش این رمان از استبداد طبقه حاکم شوروی به‌سختی انتقاد کرد و معتقد بود نظام شوروی تبدیل به دیکتاتوری شده و برپایه کیش شخصیت بنا گشته‌است. در این رمان انقلاب حیوانات مزرعه، نماد انقلاب کارگری و سرنوشت آن بر ضد نظام سرمایه‌داری است.\n",
    "بخشی از رمان: \n",
    "یکی از یک‌شنبه‌ها، صبح که حیوانات برای گرفتن دستور جمع شده بودند، ناپلئون اعلام داشت که سیاست جدیدی اتخاذ کرده است: از این تاریخ به بعد مزرعه‌ی حیوانات با مزارع مجاور دادوستد خواهد کرد، البته نه به منظور تجارت، بلکه برای به دست‌آوردن مواد مورد نیاز. گفت که آسیاب بادی باید بر هر چیز دیگری مقدم باشد. فعلاً مقداری یونجه و مقداری گندم فروخته خواهد شد و بعد اگر به پول بیش‌تری نیاز باشد از طریق فروش تخم‌مرغ، که همیشه در ولینگدن بازار دارد تامین خواهد شد. ناپلئون اضافه کرد که مرغ‌ها باید از فداکاری که به منظور کمک و شرکت در امر ساختمان آسیاب بادی ست استقبال کنند. یک بار دیگر حیوانات به شکلِ مبهمی احساس ناراحتی کردند. ارتباط نداشتن با بشر، معامله‌ی تجاری‌نکردن، پول به کار نبردن، مگر این‌ها جزو تصمیمات نخستین نشست پس از اخراج آقای جونز نبود؟ همه‌ی حیوانات این تصمیمات نخستین را به خاطر داشتند و یا لااقل تصور می‌کردند آن‌ها را به خاطر دارند.\"\"\"\n",
    "\n",
    "category_name = 'رمان'\n",
    "\n",
    "sample = preprocess_with_step_word(sample1)\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9d0042e1d158855",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd1f667d3a1ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "\n",
    "ss= 0\n",
    "for i in range(6):\n",
    "    prediction = 0\n",
    "    for word in sample:\n",
    "        prediction += predict_without_additive_smooth(word,i)\n",
    "    all_predictions.append(prediction)\n",
    "    \n",
    "print(all_predictions)\n",
    "print(all_predictions.index(max(all_predictions)))\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "ss= 0\n",
    "for i in range(6):\n",
    "    prediction = 0\n",
    "    for word in sample:\n",
    "        prediction += predict_with_additive_smooth(word,i)\n",
    "    all_predictions.append(prediction)\n",
    "    \n",
    "print(all_predictions)\n",
    "print(all_predictions.index(max(all_predictions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9917725daa1a5e1",
   "metadata": {},
   "source": [
    "that you see last has lower prediction so it is our prediction\n",
    "prediction with additive smoothing is better and correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08af3e3dae301",
   "metadata": {},
   "source": [
    "\n",
    "## Prediction function for all of words in one example and all of categories\n",
    "this fuction return prediction of each category in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c92087e20bba070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_without_additive_smooth(sample):\n",
    "    all_predictions = []\n",
    "    for i in range(6):    \n",
    "        prediction = 0\n",
    "        for word in sample:\n",
    "            prediction += predict_without_additive_smooth(word,i)\n",
    "        all_predictions.append(prediction)\n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "\n",
    "def predict_all_with_additive_smooth(sample):\n",
    "    all_predictions = []\n",
    "    for i in range(6):    \n",
    "        prediction = 0\n",
    "        for word in sample:\n",
    "            prediction += predict_with_additive_smooth(word,i)\n",
    "        all_predictions.append(prediction)\n",
    "    return all_predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a2edfd62617d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_all_without_additive_smooth(sample))\n",
    "print(predict_all_with_additive_smooth(sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c42664ea6851b36",
   "metadata": {},
   "source": [
    "## fucntion for get category name by prediction of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458ea2ceec98be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_name_by_prediction_with_additive_smooth(data):\n",
    "    data = preprocess_with_step_word(data)\n",
    "    predictions = predict_all_with_additive_smooth(data)\n",
    "    return category_data_frame['categories'][predictions.index(max(predictions))]\n",
    "\n",
    "def get_category_name_by_prediction_without_additive_smooth(data):\n",
    "    data = preprocess_with_step_word(data)\n",
    "    predictions = predict_all_without_additive_smooth(data)\n",
    "    return category_data_frame['categories'][predictions.index(max(predictions))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c1b09a435da70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_category_name_by_prediction_with_additive_smooth(sample1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de7998010011a21f",
   "metadata": {},
   "source": [
    "that was correct prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8079252ef79f10b4",
   "metadata": {},
   "source": [
    "## plot function of prediction of each category for one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599cf2547eed0f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(data):\n",
    "    data = preprocess_with_step_word(data)\n",
    "    predictions = predict_with_additive_smooth(data)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.bar(category_data_frame['categories'],predictions)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf239473261a88",
   "metadata": {},
   "source": [
    "\n",
    "## Get percentage of samples that predicted correctly\n",
    "reading samples data\n",
    "for all of three types of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49788907c774977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = pd.read_csv('books_test.csv')\n",
    "def predict_result_with_additive_smooth(data):\n",
    "    list_of_preds = []\n",
    "    for i in range(450):\n",
    "         list_of_preds.append([sample_data['categories'][i],get_category_name_by_prediction_with_additive_smooth(sample_data['description'][i]+' '+sample_data['title'][i])])\n",
    "    corrects = 0\n",
    "    for i in list_of_preds:\n",
    "        if i[0] == i[1]:\n",
    "            corrects += 1\n",
    "    return corrects/450\n",
    "\n",
    "def predict_result_without_additive_smooth(data):\n",
    "    list_of_preds = []\n",
    "    for i in range(450):\n",
    "         list_of_preds.append([sample_data['categories'][i],get_category_name_by_prediction_without_additive_smooth(sample_data['description'][i]+' '+sample_data['title'][i])])\n",
    "    corrects = 0\n",
    "    for i in list_of_preds:\n",
    "        if i[0] == i[1]:\n",
    "            corrects += 1\n",
    "    return corrects/450\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75d9235f9531288",
   "metadata": {},
   "source": [
    "## now we can get percent of correct prediction for each type of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2c8bb4bfc4695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result_with_additive_smooth_res = predict_result_with_additive_smooth(sample_data)\n",
    "print(\"percent of correct prediction with additive smooth and step word is : \",predict_result_with_additive_smooth_res)\n",
    "\n",
    "predict_result_without_additive_smooth_res = predict_result_without_additive_smooth(sample_data)\n",
    "print(\"percent of correct prediction without additive smooth and step word is : \",predict_result_without_additive_smooth_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a694fa8dee6798ab",
   "metadata": {},
   "source": [
    "## plot Result of prediction for each type of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a2de085eb1aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(['with additive smooth and step word','without additive smooth and step word'],[predict_result_with_additive_smooth_res,predict_result_without_additive_smooth_res])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff184c1adbad77",
   "metadata": {},
   "source": [
    "## we get 0.8288888888888889  percent with additive_smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86456b4c177968fd",
   "metadata": {},
   "source": [
    "difrence between phase 2 and phase 3 in unique words\n",
    "change unique words for stemmer phase 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733a80860d00fa8",
   "metadata": {},
   "source": [
    "## Phase 3 stemming  and minimizing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faffaa9af3d2c63b",
   "metadata": {},
   "source": [
    "## Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e4b732a3a3993",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_unique_words = unique_words.copy()  #for making copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b33badab674c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = main_unique_words.copy()\n",
    "\n",
    "stemmer = Stemmer()\n",
    "print(len(set(main_unique_words['words'])))\n",
    "unique_words['words'] = unique_words['words'].apply(lambda x: stemmer.stem(x))\n",
    "unique_words = unique_words.drop_duplicates(subset='words').reset_index(drop=True)\n",
    "print(len(set(unique_words['words'])))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1a19b5675824b9a9",
   "metadata": {},
   "source": [
    "update BOW for each category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0aa7b2f30b8f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(category_data_frame['categories'])):\n",
    "    BOW = [0] * len(unique_words['words'])\n",
    "    for index, word in enumerate(unique_words['words']):\n",
    "        if word in category_data_frame['data'][i]:\n",
    "            count = category_data_frame['data'][i].count(word)\n",
    "            BOW[index] += count\n",
    "    category_data_frame['BOW'][i] = BOW\n",
    "\n",
    "category_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b857db0a889bb2a",
   "metadata": {},
   "source": [
    "## now make prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879b59ea3bbddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_additive_smooth_with_stemmer():\n",
    "    sample_data= pd.read_csv('books_test.csv')\n",
    "    list_of_preds = []\n",
    "    \n",
    "    sample_data['description'] = sample_data['description'] + ' ' + sample_data['title']\n",
    "    sample_data['description'] = sample_data['description'].apply(lambda x: ' '.join([Stemmer().stem(i) for i in word_tokenize(x)]))\n",
    "    for i in range(450):\n",
    "         list_of_preds.append([sample_data['categories'][i],get_category_name_by_prediction_with_additive_smooth(sample_data['description'][i])])\n",
    "    corrects = 0 \n",
    "    for i in list_of_preds:\n",
    "        if i[0] == i[1]:\n",
    "            corrects += 1 \n",
    "    \n",
    "    return corrects/450\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b984902b80878d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result_with_additive_smooth_with_stemmer_res = predict_with_additive_smooth_with_stemmer()\n",
    "print(\"percent of correct prediction with additive smooth and step word is : \",predict_result_with_additive_smooth_with_stemmer_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44db6f654e9f169",
   "metadata": {},
   "source": [
    "## finally get  0.7533333333333333 percent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dfbada51baa5d",
   "metadata": {},
   "source": [
    "## Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7077bd71b516bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = main_unique_words.copy()\n",
    "print(len(unique_words['words']))\n",
    "lemmetize = Lemmatizer()\n",
    "unique_words['words'] = unique_words[ 'words' ].apply(lambda x: lemmetize.lemmatize(x))\n",
    "unique_words = unique_words.drop_duplicates(subset='words').reset_index(drop=True)\n",
    "\n",
    "print(len(unique_words['words']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab28c477265d7398",
   "metadata": {},
   "source": [
    "it now has 3488 words for unique words with lemmatizer\n",
    "now make update that BOW for each category for limmitize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f51a4cdb5fcec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(category_data_frame['categories'])):\n",
    "    BOW = [0] * len(unique_words['words'])\n",
    "    for index, word in enumerate(unique_words['words']):\n",
    "        if word in category_data_frame['data'][i]:\n",
    "            count = category_data_frame['data'][i].count(word)\n",
    "            BOW[index] += count\n",
    "    category_data_frame['BOW'][i] = BOW\n",
    "\n",
    "category_data_frame"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df70a67f181351ca",
   "metadata": {},
   "source": [
    "make prediction fucntion for limmitize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70985793129dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_additive_smooth_with_limmitize():\n",
    "    sample_data= pd.read_csv('books_test.csv')\n",
    "    list_of_preds = []\n",
    "    \n",
    "    sample_data['description'] = sample_data['description'] + ' ' + sample_data['title']\n",
    "    sample_data['description'] = sample_data['description'].apply(lambda x: ' '.join([lemmetize.lemmatize(i) for i in word_tokenize(x)]))\n",
    "    for i in range(450):\n",
    "         list_of_preds.append([sample_data['categories'][i],get_category_name_by_prediction_with_additive_smooth(sample_data['description'][i])])\n",
    "    corrects = 0 \n",
    "    for i in list_of_preds:\n",
    "        if i[0] == i[1]:\n",
    "            corrects += 1 \n",
    "    \n",
    "    return corrects/450\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95fc3d206117499",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result_with_additive_smooth_with_limmitize_res = predict_with_additive_smooth_with_limmitize()\n",
    "print(\"percent of correct prediction with additive smooth and step word is : \",predict_result_with_additive_smooth_with_limmitize_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8087c4ef18ee7bc",
   "metadata": {},
   "source": [
    "## finally get 0.8155555555555556 with lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "by using lemmetize and stemmer we do not have better result than without them\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "320565f0dd085b76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dc9fb8864a55a257"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
